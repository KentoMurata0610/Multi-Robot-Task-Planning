<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Multi-Robot Task Planning with Distributed On-Site Knowledge</title>
  <meta name="description" content="Project page for multi-robot task planning using LLMs and spatial concepts." />
  <link rel="stylesheet" href="assets/css/style.css" />

  <!-- Canonical / Robots -->
  <link rel="canonical" href="https://kentomurata0610.github.io/Multi-Robot-Task-Planning/">
  <meta name="robots" content="index,follow" />

  <!-- Open Graph / Twitter -->
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Multi-Robot Task Planning">
  <meta property="og:title" content="Multi-Robot Task Planning with Distributed On-Site Knowledge">
  <meta property="og:description" content="LLM-driven task decomposition and knowledge-aware allocation for cooperative object retrieval.">
  <meta property="og:url" content="https://kentomurata0610.github.io/Multi-Robot-Task-Planning/">
  <meta property="og:image" content="https://kentomurata0610.github.io/Multi-Robot-Task-Planning/assets/img/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Multi-Robot Task Planning with Distributed On-Site Knowledge">
  <meta name="twitter:description" content="LLM-driven task decomposition and knowledge-aware allocation.">
  <meta name="twitter:image" content="https://kentomurata0610.github.io/Multi-Robot-Task-Planning/assets/img/teaser.png">

  <!-- Optional: Favicon -->
  <!-- <link rel="icon" type="image/png" href="assets/img/favicon.png" /> -->

  <!-- Minimal add-on styles (safe to keep inline) -->
  <style>
    html{scroll-behavior:smooth}
    .topnav{position:sticky;top:0;z-index:50;background:rgba(255,255,255,.85);backdrop-filter:saturate(180%) blur(6px);border-bottom:1px solid #eee}
    .topnav .navwrap{display:flex;gap:14px;padding:10px 0}
    .topnav a{color:#111;font-weight:600;opacity:.85;text-decoration:none}
    .topnav a:hover{opacity:1}
    /* Responsive video container */
    .video-container{position:relative;padding-bottom:56.25%;height:0}
    .video-container iframe{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:12px;border:1px solid #eee}
    /* Minor readability tweaks */
    .hero .authors{color:#666;max-width:820px}
    .caption{color:#666}
    /* Dark mode */
    @media (prefers-color-scheme: dark){
      .topnav{background:rgba(12,16,22,.75);border-bottom-color:#203040}
      .video-container iframe{border-color:#203040;background:#0a0f14}
    }
  </style>

  <!-- Structured data (optional) -->
  <script type="application/ld+json">
  {
    "@context":"https://schema.org",
    "@type":"ScholarlyArticle",
    "name":"Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models",
    "author":[
      {"@type":"Person","name":"Kento Murata"},
      {"@type":"Person","name":"Shoichi Hasegawa"},
      {"@type":"Person","name":"Tomochika Ishikawa"},
      {"@type":"Person","name":"Yoshinobu Hagiwara"},
      {"@type":"Person","name":"Akira Taniguchi"},
      {"@type":"Person","name":"Lotfi El Hafi"},
      {"@type":"Person","name":"Tadahiro Taniguchi"}
    ],
    "url":"https://kentomurata0610.github.io/Multi-Robot-Task-Planning/",
    "sameAs":"https://arxiv.org/abs/2509.12838",
    "image":"https://kentomurata0610.github.io/Multi-Robot-Task-Planning/assets/img/teaser.png",
    "datePublished":"2025-09-18"
  }
  </script>
</head>
<body>
  <header class="hero">
    <div class="container">
      <h1>Multi-Robot Task Planning for Multi-Object Retrieval with Distributed On-Site Knowledge via LLMs</h1>
      <p class="authors">
        Kento Murata, Shoichi Hasegawa, Tomochika Ishikawa, Yoshinobu Hagiwara, Akira Taniguchi, Lotfi El Hafi, Tadahiro Taniguchi
      </p>
      <p class="links">
        <a class="btn" href="https://arxiv.org/abs/2509.12838" target="_blank" aria-label="Open arXiv paper">üìÑ Paper (arXiv)</a>
        <a class="btn" href="papers/AROB_ISBC_2026_Murata.pdf" target="_blank" aria-label="Open submission PDF">üìÑ Submission PDF (under review)</a>
        <a class="btn" href="https://youtu.be/LMoWAp_kPhk" target="_blank" aria-label="Open demo video">‚ñ∂Ô∏è Demo Video</a>
      </p>
      <img class="teaser"
           src="assets/img/teaser.png"
           alt="Overview figure"
           width="1400" height="700"
           loading="lazy" decoding="async" />
      <p class="caption">Overview: natural-language instructions are decomposed into subtasks and allocated to robots using on-site spatial knowledge.</p>
    </div>
  </header>

  <!-- Sticky local navigation -->
  <nav class="topnav" aria-label="Section navigation">
    <div class="container navwrap">
      <a href="#abstract">Abstract</a>
      <a href="#method">Method</a>
      <a href="#experiments">Experiments</a>
      <a href="#resources">Resources</a>
      <a href="#demo">Demo</a>
      <a href="#bibtex">BibTeX</a>
      <a href="#ack">Acknowledgments</a>
    </div>
  </nav>

  <main class="container">
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        We study cooperative object search by multiple robots that receive natural-language instructions including multi-object or context-dependent goals (e.g., ‚Äúfind an apple and a banana‚Äù).
        Our framework integrates a large language model (LLM) with a spatial concept model that provides room names and room-wise object presence probabilities learned on each robot‚Äôs assigned area.
        With a tailored prompting strategy, the LLM infers required items from ambiguous commands, decomposes them into subtasks, and allocates them to robots that are most likely to succeed given their local knowledge.
        In experiments, the method achieved <strong>47/50 successful allocations</strong>, outperforming random (28/50) and commonsense-only allocation (26/50), and was validated qualitatively on real mobile manipulators. <span class="cite">[See paper]</span>
      </p>
    </section>

    <section id="method">
      <h2>Method</h2>
      <p>
        Each robot learns on-site knowledge via a spatial concept model that links places (e.g., kitchen, bedroom) and object occurrence probabilities.
        The pipeline has four stages: (1) task decomposition from language, (2) knowledge-aware subtask allocation, (3) sequential action planning (navigation, object detection, pick, place), and (4) execution with feedback loops (FlexBE).
      </p>
      <img src="assets/img/pipeline.png"
           alt="Pipeline diagram"
           width="1400" height="780"
           loading="lazy" decoding="async" />
      <p class="caption">Four-stage pipeline: knowledge acquisition ‚Üí decomposition &amp; allocation ‚Üí action planning ‚Üí execution with feedback.</p>
      <ul>
        <li><strong>On-site knowledge</strong>: place vocabulary + object‚Üîlocation probabilities</li>
        <li><strong>LLM prompts</strong>: few-shot design to infer items, decompose tasks, and justify allocation</li>
        <li><strong>Execution</strong>: closed-loop behaviors and replanning on success/failure signals</li>
      </ul>
    </section>

    <section id="experiments">
      <h2>Experiments</h2>
    
      <!-- First/Second floor side-by-side on desktop -->
      <div class="media-grid two">
        <figure>
          <img src="assets/img/1_floor_env.png" alt="Evaluation environment ‚Äì First floor" loading="lazy" decoding="async">
          <figcaption>Evaluation environment ‚Äì First floor (5 rooms, 12 objects).</figcaption>
        </figure>
    
        <figure>
          <img src="assets/img/2_floor_env.png" alt="Evaluation environment ‚Äì Second floor" loading="lazy" decoding="async">
          <figcaption>Evaluation environment ‚Äì Second floor (5 rooms, 12 objects).</figcaption>
        </figure>
      </div>
    
      <p>
        We evaluate allocation accuracy across instruction types (random, hard-to-predict, common-sense, mixed).
        The proposed method reaches <strong>47/50</strong> correct allocations versus <em>random</em> <strong>28/50</strong> and <em>commonsense-only</em> <strong>26/50</strong>.
      </p>
    
      <figure>
        <img src="assets/img/results.png" alt="Results summary" loading="lazy" decoding="async">
        <figcaption>Allocation success counts across instruction types (random/hard/common-sense/mixed). See Table 3 in the paper.</figcaption>
      </figure>
    </section>
    
    <section id="resources">
      <h2>Resources</h2>
      <ul>
        <li>Paper (arXiv): <a href="https://arxiv.org/abs/2509.12838" target="_blank">arXiv:2509.12838</a></li>
        <li>Submission PDF (AROB/ISBC 2026, under review): <a href="papers/AROB_ISBC_2026_Murata.pdf" target="_blank">PDF</a></li>
        <li>Video: <a href="https://youtu.be/LMoWAp_kPhk" target="_blank">Demo</a></li>
      </ul>
      <p class="caption"><strong>Code availability:</strong> not publicly released at this time.</p>
    </section>

    <section id="demo">
      <h2>Demo Video</h2>
      <div class="video-container">
        <iframe
          src="https://www.youtube.com/embed/LMoWAp_kPhk"
          title="Demo video"
          frameborder="0"
          loading="lazy"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen>
        </iframe>
      </div>
      <p class="caption">Demonstration of our multi-robot task planning framework.</p>
    </section>

    <section id="bibtex">
      <h2>BibTeX</h2>
      <button class="btn" id="copyBib" type="button" aria-label="Copy BibTeX">Copy BibTeX</button>
      <pre class="bibtex" id="bibtexBlock">@article{Murata2025MultiRobotTaskPlanning,
  title   = {Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models},
  author  = {Murata, Kento and Hasegawa, Shoichi and Ishikawa, Tomochika and Hagiwara, Yoshinobu and Taniguchi, Akira and El Hafi, Lotfi and Taniguchi, Tadahiro},
  journal = {arXiv preprint arXiv:2509.12838},
  year    = {2025}
}
</pre>
      <p class="caption">Please cite the arXiv version until the conference review is complete.</p>
    </section>

    <section id="ack">
      <h2>Acknowledgments</h2>
      <p>
        Partially supported by JST Moonshot (JPMJMS2011), JSPS KAKENHI (JP25K15292, JP23K16975), and JST Challenging Research Program for Next-Generation Researchers (JPMJSP2101).
      </p>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <p>¬© 2026 The Authors. This page summarizes findings from the paper; please cite the paper if you use any part of this work.</p>
    </div>
  </footer>

  <!-- Copy-to-clipboard for BibTeX -->
  <script>
    (function () {
      var btn = document.getElementById('copyBib');
      var pre = document.getElementById('bibtexBlock');
      if (!btn || !pre) return;
      btn.addEventListener('click', async function () {
        try {
          await navigator.clipboard.writeText(pre.textContent.trim());
          btn.textContent = 'Copied!';
          setTimeout(()=> btn.textContent = 'Copy BibTeX', 1600);
        } catch(e) { btn.textContent = 'Copy failed'; }
      });
    })();
  </script>
</body>
</html>
